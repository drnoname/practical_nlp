{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. NLP Primer\n",
    "\n",
    "\n",
    "计算机只能理解0和1，所以NLP 的主要问题是，怎么样将自然语言以0和1的形式给程序，并理解。\n",
    "NLP deals with methods to analyze, model, and understand human language\n",
    "\n",
    "\n",
    "## 1. NLP tasks and applications\n",
    "\n",
    "<img src=\"../figures/1-1.png\" alt=\"drawing\" width=\"800\"/>\n",
    "\n",
    "### 1.1 NLP applications\n",
    "- **Email platforms**: spam classification, priority inbox, calendar event extraction, auto-complete, etc. \n",
    "- **Voice-based assistants**: interact with the user, understand user commands, and respond accordingly.\n",
    "- **Search engine**: query understanding, query expansion, question answering, information retrieval, and ranking and grouping of the results\n",
    "- **machine translation**\n",
    "- **social media**: sentimental analysis\n",
    "- **E-Commerce platforms**: understanding customer reviews\n",
    "- **Text generation**: reports\n",
    "- **Spelling check**\n",
    "- **Plagiarism detection**\n",
    "- **Build large knowledge bases**: knowledge base / graph is used for search and QA.\n",
    "- many others...\n",
    "\n",
    "### 1.2 NLP tasks\n",
    "\n",
    "上述NLP 问题大多可以分解成以下tasks:\n",
    "\n",
    "- Language modeling:\n",
    "    - The goal of this task is to learn the probability of a sequence of words appearing in a given language.\n",
    "    - 使用场景: speech recognition, OCR (Optical Character Recognition), handwriting recognition, machine translation, spelling correction, etc.\n",
    "\n",
    "- Text classification\n",
    "    - 使用场景: email spam identification, sentiment analysis\n",
    "    \n",
    "- Information extraction\n",
    "    - 使用场景: calendar events, etc.\n",
    "    \n",
    "- Information retrieval\n",
    "    - This is the task of finding documents relevant to a user query from a large collection. \n",
    "    - 使用场景: Google search\n",
    "    \n",
    "- Conversational agent\n",
    "    - Siri, Alexa, etc.\n",
    "\n",
    "- Text summarization\n",
    "    - Create short summaries of longer documents while retaining the core content and preserving the overall meaning of the text.\n",
    "\n",
    "- Question answering\n",
    "\n",
    "- Machine translation\n",
    "    - 使用场景: Google translate\n",
    "\n",
    "- Topic modeling\n",
    "    - This is the task of uncovering the topical structure of a large collection of documents. \n",
    "\n",
    "下面，我们按这些task 的复杂度进行排序\n",
    "\n",
    "<img src=\"../figures/1-2.png\" alt=\"drawing\" width=\"600\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Building blocks of language?\n",
    "\n",
    "自然语言主要有4个组成部分:\n",
    "\n",
    "<img src=\"../figures/1-3.png\" alt=\"drawing\" width=\"800\"/>\n",
    "\n",
    "\n",
    "### 2.1 phonemes\n",
    "important in speech understanding.\n",
    "- smallest units of sound\n",
    "- may not have meaning\n",
    "- can induce meanings: un, in, etc.\n",
    "\n",
    "### 2.2 morphemes and lexemes\n",
    "A morpheme is the smallest unit of language that has a meaning. multimedia, multi is a morpheme.\n",
    "\n",
    "<img src=\"../figures/1-5.png\" alt=\"drawing\" width=\"600\"/>\n",
    "\n",
    "Lexemes are the structural variations of morphemes related to one another by meaning. “run” and “running” belong to the same lexeme form.\n",
    "\n",
    "\n",
    "### 2.3 syntax\n",
    "Syntax is a set of rules to construct grammatically correct sentences out of words and phrases in a language.\n",
    "syntactic structure 通常用一个parsing tree 来表示(**N** stands for noun, **V** for verb, and **P** for preposition, Noun phrase is denoted by **NP** and verb phrase by **VP**).\n",
    "\n",
    "<img src=\"../figures/1-6.png\" alt=\"drawing\" width=\"600\"/>\n",
    "\n",
    "Entity extraction and relation extraction are some of the NLP tasks that build on this knowledge of parsing.\n",
    "\n",
    "### 2.4 context\n",
    "\n",
    "Context is how various parts in a language come together to convey a particular meaning. The meaning of a sentence can change based on the context, as words and phrases can sometimes have multiple meanings. context 包括semantics and pragmatics.\n",
    "\n",
    "- semantics: the direct meaning of the words and sentences without external context\n",
    "- pragmatics: adds world knowledge and external context of the conversation to enable us to infer implied meaning\n",
    "\n",
    "使用场景: sarcasm detection, summarization, topic modeling. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Why is NLP challenging\n",
    "\n",
    "### 3.1 Ambiguity\n",
    "\n",
    "Ambiguity = uncertainty of meaning, 参考下面的例子(from **Winograd Schema Challenge**)。\n",
    "\n",
    "<img src=\"../figures/1-7.png\" alt=\"drawing\" width=\"600\"/>\n",
    "\n",
    "因为每一对句子都只有一两个词不一样，所以使用大部分NLP 方法，很难区分他们。但是人类可以轻松区分他们。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Common Knowledge\n",
    "\n",
    "人们在谈话的时候，有一些common knowledge 是大家默认都知道的，所以不需要exlicitly mention. \n",
    "\n",
    "One of the key challenges in NLP is how to encode all the things that are common knowledge to humans in a computational model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Creativity\n",
    "\n",
    "Various styles, dialects, genres, and variations are used in any language. \n",
    "\n",
    "\n",
    "### 3.4 Diversity across languages\n",
    "\n",
    "当我们有一个NLP 系统时，我们想把它用于另一个language，会遇到困难。因为For most languages in the world, there is no direct mapping between the vocabularies of any two languages.\n",
    "\n",
    "对于一个语言的解决方案，对于另一个语言可能不适用。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Machine Learning, Deep Learning, and NLP: An Overview\n",
    "\n",
    "下面我们来看看一些解决NLP 问题的通用方法。\n",
    "\n",
    "machine learning，deep learning 和NLP之间的关系如下图所示：\n",
    "\n",
    "<img src=\"../figures/1-8.png\" alt=\"drawing\" width=\"600\"/>\n",
    "\n",
    "- **Supervised Learning**: The goal is to learn the mapping function from input to output given a large number of examples in the form of input-output pairs.\n",
    "\n",
    "- **Unsupervised Learning**: aim to find hidden patterns in given input data without any reference output.\n",
    "\n",
    "- **Reinforcement Learning**: learn tasks via trial and error and is characterized by the absence of either labeled or unlabeled data in large quantities. (现在在NLP 领域还没有大规模使用)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Approaches to NLP\n",
    "\n",
    "通常分为三类：\n",
    "- heuristics\n",
    "- machine learning\n",
    "- deep learning\n",
    "\n",
    "### 5.1 heuristics-based NLP\n",
    "\n",
    "Rule-based. \n",
    "Example: lexicon-based sentiment analysis, count of positive and negative words.\n",
    "\n",
    "#### heristics-based 方法的优势是什么?\n",
    "- Put simply, rules and heuristics help you quickly build the first version of the model and get a better understanding of the problem at hand.\n",
    "- Rules and heuristics can also be useful as features for machine learning–based NLP systems.\n",
    "- 多一些确定性，某些行业对可靠性要求更高，例如health care\n",
    "\n",
    "#### 通常使用工具：\n",
    "- 字典\n",
    "- 知识库(例如wordnet等)\n",
    "    - Synonyms: refer to words with similar meanings\n",
    "    - Hyponyms: capture is-type-of relationships.\n",
    "        - baseball, tennis are hyponyms of sports\n",
    "    - Meronyms capture is-part-of relationships\n",
    "        - hands and legs are meronyms of the body\n",
    "\n",
    "<img src=\"../figures/1-9.png\" alt=\"drawing\" width=\"500\"/>\n",
    "\n",
    "- Regex\n",
    "    - build rule-based system 通常使用Regex. \n",
    "    - Regexes are a great way to incorporate domain knowledge in your NLP system.\n",
    "    - **Probabilistic regexes** “including a probability of a match (参见pregex library)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Machine Learning for NLP\n",
    "\n",
    "- **classification**: classify an article to news topics\n",
    "- **regression**: predict stock price based on relevant news (social media discussion and rumors)\n",
    "- **clustering**: group similar documents\n",
    "\n",
    "所有机器学习的方法分为三步：\n",
    "1. extract features from text -- (chapter 3)\n",
    "2. using features to learn a model\n",
    "    - **Naive Bayes**\n",
    "        - assumes each feature is independent of all other features\n",
    "        - Pros: simple to understand, fast to train and run\n",
    "        - Cons: strong assumption -- usually used as the starting algorithms for text classification\n",
    "    - **SVM**\n",
    "        - To learn a decision boundary that acts as a separation between different categories of text\n",
    "        - Pros: robustness to variation and noise in the data\n",
    "        - Cons: training time, inability to scale\n",
    "    - **Hiddel Markov Model**\n",
    "        - assumes there is an underlying, unobservable process with hidden states that generates the data. “each hidden state is dependent on the previous state(s).\n",
    "        - POS tagging: underlying grammar rules\n",
    "    - **Conditional Random Fields**\n",
    "        - performs a classification task on each element in the sequence\n",
    "        - CRFs outperform HMMs for tasks such as POS tagging\n",
    "3. evaluating and improving the model -- (chapter 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Deep learning for NLP\n",
    "\n",
    "#### RNN\n",
    "The memory is temporal\n",
    "\n",
    "<img src=\"../figures/1-13.png\" alt=\"drawing\" width=\"500\"/>\n",
    "\n",
    "RNN is useful for text classification, named entity recognition, machine translation, text generation, etc.\n",
    "\n",
    "\n",
    "#### LSTM\n",
    "RNN suffers from forgetful memory — they cannot remember longer contexts and therefore do not perform well when the input text is long.\n",
    "\n",
    "LSTM 丢弃掉无用的信息，保留有用的context。这个memory 用一个vector 表示。\n",
    "\n",
    "#### GRU\n",
    "相比LSTM，更多用于text generation。\n",
    "\n",
    "#### CNN\n",
    "常用于text-classification tasks\n",
    "\n",
    "The main advantage CNNs have is their ability to look at a group of words together using a context window.\n",
    "\n",
    "<img src=\"../figures/1-15.png\" alt=\"drawing\" width=\"600\"/>\n",
    "\n",
    "#### Transformers\n",
    "They model the textual context but not in a sequential manner\n",
    "\n",
    "large transformers have been used for transfer learning with smaller downstream tasks. \n",
    "- pre-training: \n",
    "- fine-tuning: fine-tuned on downstream NLP tasks, such as text classification, entity extraction, question answering\n",
    "\n",
    "<img src=\"../figures/1-16.png\" alt=\"drawing\" width=\"600\"/>\n",
    "\n",
    "BERT.\n",
    "\n",
    "<img src=\"../figures/1-17.png\" alt=\"drawing\" width=\"600\"/>\n",
    "\n",
    "#### Autoencoders\n",
    "learning compressed vector representation of the input. Autoencoders are typically used to create feature representations needed for any downstream tasks.\n",
    "\n",
    "<img src=\"../figures/1-18.png\" alt=\"drawing\" width=\"600\"/>\n",
    "\n",
    "LSTM auto-encoders.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Deep learning is not yet the silver bullet for NLP\n",
    "\n",
    "deep learning在industry 实际落地场景中存在有一些新的挑战:\n",
    "\n",
    "#### Overfitting on small datasets\n",
    "\n",
    "Occam’s razor suggests that a simpler solution is always preferable given that all other conditions are equal. \n",
    "\n",
    "当数据不足时，简单的模型更好。\n",
    "\n",
    "#### Few-shot learning and synthetic data generation\n",
    "\n",
    "在图像处理领域，已经可以通过少数的样本训练生成较好的模型。但在NLP 领域还做不到。\n",
    "\n",
    "#### Domain adaptation\n",
    "\n",
    "例如法律，both the syntactic and semantic structure of the language is specific to the domain. transfer 的时候会造成performance degradation.\n",
    "\n",
    "#### Interpretable models\n",
    "\n",
    "Controllability and interpretability is hard for DL models because, most of the time, they work like a black box.\n",
    "\n",
    "在图像处理上，DL model 不是那么的blackbox.\n",
    "\n",
    "#### Common sense and world knowledge\n",
    "\n",
    "科学家对于language 本身的理解，并不透彻。人们在使用语言时，同时有logical reasoning, 对计算机很难。\n",
    "\n",
    "现在的DL model 对于common sense understanding and logical reasoning 表现困难。如何把knowledge (e.g., knowledge graph) 和DL model integrate？\n",
    "\n",
    "#### Cost\n",
    "\n",
    "In terms of both money and time.\n",
    "\n",
    "除此以外，bulky models may cause latency issues during inference time and may not be useful in cases where low latency is a must.\n",
    "\n",
    "#### On-device deployment\n",
    "\n",
    "owing to limitations of the device, the solution must work with limited memory and power.\n",
    "\n",
    "基于以上挑战，我们可以看出，DL is not always the go-to solution for all industrial NLP applications.\n",
    "\n",
    "本书会介绍pipeline，而并非以两种DL 模型。\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. An NLP walkthrough: conversational agents\n",
    "\n",
    "Siri, Alexa. 包括以下几个major NLP components.\n",
    "\n",
    "<img src=\"../figures/1-19.png\" alt=\"drawing\" width=\"500\"/>\n",
    "\n",
    "#### Speech recognition and synthesis\n",
    "\n",
    "获取用户输入：voice-based conversational agent 必备。\n",
    "\n",
    "- Speech recognition: speech signals to text\n",
    "- Speech synthesis: text to speech\n",
    "\n",
    "这两项技术现在都比较成熟，通常的做法是使用Cloud API.\n",
    "\n",
    "#### Natural language understanding\n",
    "\n",
    "理解用户的输入，可分为以下几个组件：\n",
    "\n",
    "1. Sentiment Analysis\n",
    "analyze the sentiment of the user response (chapter 4)\n",
    "\n",
    "2. Named Entity Recognition\n",
    "identify all the important entities the user mentioned in their response (chapter 5)\n",
    "\n",
    "3. Coreference resolution\n",
    "find out the references of the extracted entities from the conversation history. (chapter 5)\n",
    "\n",
    "#### Dialog management\n",
    "\n",
    "1. Once we’ve extracted the useful information from the user’s response, we may want to understand the user’s intent.\n",
    "    - **Intent classification**: We can use a text-classification system to classify the user response as one of the pre-defined intents.\n",
    "\n",
    "2. Once we’ve figured out the user’s intent, we want to figure out which suitable action the conversational agent should take to fulfill the user’s request.\n",
    "\n",
    "#### Response generation\n",
    "Finally, the conversational agent generates a suitable action to perform based on a semantic interpretation of the user’s intent and additional inputs from the dialogue with the user. \n",
    "\n",
    "- retrieve information from the knowledge base\n",
    "- generate response using a pre-defined template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
