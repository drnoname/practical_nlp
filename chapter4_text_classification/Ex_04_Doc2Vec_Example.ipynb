{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LCgVnQopb6TI"
   },
   "source": [
    "# Doc2Vec demonstration \n",
    "\n",
    "In this notebook, let us take a look at how to \"learn\" **document embeddings** and use them for text classification. We will be using the dataset of \"Sentiment and Emotion in Text\" from [Kaggle](https://www.kaggle.com/c/sa-emotions/data).\n",
    "\n",
    "\"In a variation on the popular task of sentiment analysis, this dataset contains labels for the emotional content (such as happiness, sadness, and anger) of texts. Hundreds to thousands of examples across 13 labels. A subset of this data is used in an experiment we uploaded to Microsoft’s Cortana Intelligence Gallery.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "hSB6W1seb6TJ",
    "outputId": "fa6730c6-06df-46ce-91c8-cd59211d24de"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 470
    },
    "colab_type": "code",
    "id": "Fnv168QRJjxi",
    "outputId": "a99d7d74-d248-4fa0-bf66-916283ffd401"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-08-10 11:36:06--  https://raw.githubusercontent.com/practical-nlp/practical-nlp/master/Ch4/Data/Sentiment%20and%20Emotion%20in%20Text/train_data.csv\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 2479133 (2.4M) [text/plain]\n",
      "Saving to: ‘DATAPATH/train_data.csv’\n",
      "\n",
      "\r",
      "train_data.csv        0%[                    ]       0  --.-KB/s               \r",
      "train_data.csv      100%[===================>]   2.36M  14.7MB/s    in 0.2s    \n",
      "\n",
      "2020-08-10 11:36:07 (14.7 MB/s) - ‘DATAPATH/train_data.csv’ saved [2479133/2479133]\n",
      "\n",
      "--2020-08-10 11:36:08--  https://raw.githubusercontent.com/practical-nlp/practical-nlp/master/Ch4/Data/Sentiment%20and%20Emotion%20in%20Text/test_data.csv\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 783640 (765K) [text/plain]\n",
      "Saving to: ‘DATAPATH/test_data.csv’\n",
      "\n",
      "test_data.csv       100%[===================>] 765.27K  --.-KB/s    in 0.1s    \n",
      "\n",
      "2020-08-10 11:36:09 (6.78 MB/s) - ‘DATAPATH/test_data.csv’ saved [783640/783640]\n",
      "\n",
      "total 3.2M\n",
      "drwxr-xr-x 2 root root 4.0K Aug 10 11:36 .\n",
      "drwxr-xr-x 1 root root 4.0K Aug 10 11:36 ..\n",
      "-rw-r--r-- 1 root root 766K Aug 10 11:36 test_data.csv\n",
      "-rw-r--r-- 1 root root 2.4M Aug 10 11:36 train_data.csv\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "#downloding data\n",
    "!wget -P DATAPATH https://raw.githubusercontent.com/practical-nlp/practical-nlp/master/Ch4/Data/Sentiment%20and%20Emotion%20in%20Text/train_data.csv\n",
    "!wget -P DATAPATH https://raw.githubusercontent.com/practical-nlp/practical-nlp/master/Ch4/Data/Sentiment%20and%20Emotion%20in%20Text/test_data.csv\n",
    "!ls -lah DATAPATH\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 212
    },
    "colab_type": "code",
    "id": "lSvnHBYPb6TQ",
    "outputId": "e2aac8d5-ef66-4e02-9949-32434f8cb537"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30000, 2)\n"
     ]
    }
   ],
   "source": [
    "#Load the dataset and explore.\n",
    "data_path = '/Users/chenwang/Workspace/github/practical_nlp/chapter4_text_classification/data/Sentiment_and_Emotion_in_Text/'\n",
    "filepath = data_path + \"train_data.csv\"\n",
    "df = pd.read_csv(filepath)\n",
    "print(df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>empty</td>\n",
       "      <td>@tiffanylue i know  i was listenin to bad habi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sadness</td>\n",
       "      <td>Layin n bed with a headache  ughhhh...waitin o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sadness</td>\n",
       "      <td>Funeral ceremony...gloomy friday...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>enthusiasm</td>\n",
       "      <td>wants to hang out with friends SOON!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>neutral</td>\n",
       "      <td>@dannycastillo We want to trade with someone w...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    sentiment                                            content\n",
       "0       empty  @tiffanylue i know  i was listenin to bad habi...\n",
       "1     sadness  Layin n bed with a headache  ughhhh...waitin o...\n",
       "2     sadness                Funeral ceremony...gloomy friday...\n",
       "3  enthusiasm               wants to hang out with friends SOON!\n",
       "4     neutral  @dannycastillo We want to trade with someone w..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 252
    },
    "colab_type": "code",
    "id": "5JEI6SH7b6TU",
    "outputId": "22cc98a5-90d0-49c9-fb58-f40d743963e9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "worry         7433\n",
       "neutral       6340\n",
       "sadness       4828\n",
       "happiness     2986\n",
       "love          2068\n",
       "surprise      1613\n",
       "hate          1187\n",
       "fun           1088\n",
       "relief        1021\n",
       "empty          659\n",
       "enthusiasm     522\n",
       "boredom        157\n",
       "anger           98\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们可以看出，一共有30000 条数据，共13种不同的sentiment，下面我们选择三个情绪，对以后的document 进行分类。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "CHajyKpmb6TY",
    "outputId": "8749211d-1a7c-43c9-bf40-d4d22e74407a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16759, 2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Let us take the top 3 categories and leave out the rest.\n",
    "shortlist = ['neutral', \"happiness\", \"worry\"]\n",
    "df_subset = df[df['sentiment'].isin(shortlist)]\n",
    "df_subset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看出，进过筛选，我们有16759 条数据，总共3个sentiments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m2oiZzU5b6Tf"
   },
   "source": [
    "# Text pre-processing:\n",
    "Tweets are different. Somethings to consider:\n",
    "- Removing @mentions, and urls perhaps?\n",
    "- using NLTK Tweet tokenizer instead of a regular one\n",
    "    - ;-) should be 1 token rather than 3\n",
    "- stopwords, numbers as usual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Rl-FfMdLb6Th",
    "outputId": "6273576c-0495-4606-da02-a7d06358ab2d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16759 16759\n"
     ]
    }
   ],
   "source": [
    "# strip_handles removes personal information such as twitter handles, which don't contribute to emotion in the tweet. \n",
    "# preserve_case=False converts everything to lowercase.\n",
    "tweeterTokenizer = TweetTokenizer(strip_handles=True,preserve_case=False)\n",
    "mystopwords = set(stopwords.words(\"english\"))\n",
    "\n",
    "# Function to tokenize tweets, remove stopwords and numbers. \n",
    "# Keeping punctuations and emoticon symbols could be relevant for this task!\n",
    "def preprocess_corpus(texts):\n",
    "    def remove_stops_digits(tokens):\n",
    "        #Nested function that removes stopwords and digits from a list of tokens\n",
    "        return [token for token in tokens if token not in mystopwords and not token.isdigit()]\n",
    "    #This return statement below uses the above function to process twitter tokenizer output further. \n",
    "    return [remove_stops_digits(tweeterTokenizer.tokenize(content)) for content in texts]\n",
    "\n",
    "#df_subset contains only the three categories we chose. \n",
    "mydata = preprocess_corpus(df_subset['content'])\n",
    "mycats = df_subset['sentiment']\n",
    "print(len(mydata), len(mycats))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "colab_type": "code",
    "id": "rsGwfVebb6Tl",
    "outputId": "ce668f41-578a-467e-d3e7-20ea66b53c6d"
   },
   "outputs": [],
   "source": [
    "# Split data into train and test, following the usual process\n",
    "train_data, test_data, train_cats, test_cats = train_test_split(mydata,mycats,random_state=1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### converting the data into a format readable by the Doc2vec implementation\n",
    "\n",
    "It’s used to represent a document as a list of tokens, followed by a “tag,” which in its simplest form can be just the filename or ID of the document. 这里我们使用document 的ID（第几个docuemnt）.\n",
    "\n",
    "知道了做什么，但是不清楚为什么要这么做。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare training data in doc2vec format:\n",
    "train_doc2vec = [TaggedDocument((d), tags=[str(i)]) for i, d in enumerate(train_data)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 训练\n",
    "\n",
    "参数:\n",
    "- `vector_size`: the dimensionality of the learned embeddings; \n",
    "- `alpha`: the learning rate; \n",
    "- `min_count` the minimum frequency of words that remain in vocabulary; \n",
    "- `dm`: distributed memory, is one of the representation learners implemented in Doc2vec (the other is `dbow`, or distributed bag of words); \n",
    "- `epochs`: the number of training iterations.\n",
    "\n",
    "如何调整参数: Lau, Jey Han and Timothy Baldwin. “An Empirical Evaluation of doc2vec with Practical Insights into Document Embedding Generation”. (2016).\n",
    "\n",
    "最好的选参方法是**grid search**.\n",
    "The best way to address this issue is to explore a range of values for the ones that matter to us (e.g., dm versus dbow, vector sizes, learning rate) and compare multiple models.\n",
    "\n",
    "因为model 的输出是一个document vector, 所以如何对比两组参数的结果哪一个更好？一个方法是使用downstream application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 45s, sys: 18.8 s, total: 2min 3s\n",
      "Wall time: 1min 38s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#Train a doc2vec model to learn tweet representations. Use only training data!!\n",
    "model = Doc2Vec(vector_size=50, alpha=0.025, min_count=5, dm =1, epochs=100)\n",
    "model.build_vocab(train_doc2vec)\n",
    "model.train(train_doc2vec, total_examples=model.corpus_count, epochs=model.epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Saved\n"
     ]
    }
   ],
   "source": [
    "model.save(\"output/d2v.model\")\n",
    "print(\"Model Saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "colab_type": "code",
    "id": "hTqo26Vsb6Ts",
    "outputId": "13f5218a-a22d-400d-bd9e-d53a51c767d7"
   },
   "outputs": [],
   "source": [
    "#Infer the feature representation for training and test data using the trained model\n",
    "model= Doc2Vec.load(\"output/d2v.model\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "there is some amount of randomness due to the choice of hyperparameters, the inferred vectors differ each time we extract them. For this reason, to get a stable representation, we run it multiple times (called **steps**) and aggregate the vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#infer in multiple steps to get a stable representation. \n",
    "train_vectors =  [model.infer_vector(list_of_tokens, steps=50) for list_of_tokens in train_data]\n",
    "test_vectors = [model.infer_vector(list_of_tokens, steps=50) for list_of_tokens in test_data]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FQuoinmiK_jG"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chenwang/anaconda3/envs/tf/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/chenwang/anaconda3/envs/tf/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "   happiness       0.38      0.44      0.41       713\n",
      "     neutral       0.46      0.55      0.50      1595\n",
      "       worry       0.58      0.45      0.51      1882\n",
      "\n",
      "   micro avg       0.49      0.49      0.49      4190\n",
      "   macro avg       0.47      0.48      0.47      4190\n",
      "weighted avg       0.50      0.49      0.49      4190\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Use any regular classifier like logistic regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "myclass = LogisticRegression(class_weight=\"balanced\") #because classes are not balanced. \n",
    "myclass.fit(train_vectors, train_cats)\n",
    "\n",
    "preds = myclass.predict(test_vectors)\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(classification_report(test_cats, preds))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[314 262 137]\n",
      " [233 873 489]\n",
      " [272 754 856]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(test_cats,preds))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从上面的结果看，准确率很低，分析原因如下：\n",
    "- 很多短文，比新闻等长文的难度大\n",
    "- emoticons, spelling, etc.\n",
    "\n",
    "#### Tips\n",
    "\n",
    "An important point to keep in mind when using Doc2vec is the same as for fastText: if we have to use Doc2vec for feature representation, we have to store the model that learned the representation. While it’s not typically as bulky as fastText, it’s also not as fast to train. Such trade-offs need to be considered and compared before we make a deployment decision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Doc2Vec_Example.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
