{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sVtvH58nb_Hp"
   },
   "source": [
    "# Word2Vec for Text Classification\n",
    "\n",
    "In this short notebook, we will see an example of how to use a pre-trained Word2vec model for doing feature extraction and performing text classification.\n",
    "\n",
    "We will use the sentiment labelled sentences dataset from UCI repository\n",
    "http://archive.ics.uci.edu/ml/datasets/Sentiment+Labelled+Sentences\n",
    "\n",
    "The dataset consists of 1500 positive, and 1500 negative sentiment sentences from Amazon, Yelp, IMDB. Let us first combine all the three separate data files into one using the following unix command:\n",
    "\n",
    "```cat amazon_cells_labelled.txt imdb_labelled.txt yelp_labelled.txt > sentiment_sentences.txt```\n",
    "\n",
    "For a pre-trained embedding model, we will use the Google News vectors.\n",
    "https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM\n",
    "\n",
    "Let us get started!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JQX8DAmBb_Hr"
   },
   "outputs": [],
   "source": [
    "# basic imports\n",
    "import os\n",
    "from time import time\n",
    "\n",
    "# pre-processing imports\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "\n",
    "# imports related to modeling\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. load word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "COUGXAxcb_H5",
    "outputId": "f1b6d8ad-e22b-4126-d2ea-862697c4158b",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Load the pre-trained word2vec model and the dataset\n",
    "data_path = \"/Users/chenwang/Workspace/github/practical_nlp/chapter4_text_classification/data/sentiment_labelled_sentences\"\n",
    "model_path = \"/Users/chenwang/Workspace/datasets/models\"\n",
    "path_to_model = os.path.join(model_path,'GoogleNews-vectors-negative300.bin')\n",
    "training_data_path = os.path.join(data_path, \"sentiment_sentences.txt\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 34s, sys: 6.36 s, total: 1min 41s\n",
      "Wall time: 1min 49s\n",
      "done loading Word2Vec\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Load W2V model. This will take some time. \n",
    "%time w2v_model = KeyedVectors.load_word2vec_format(path_to_model, binary=True)\n",
    "print('done loading Word2Vec')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. 读入原始数据，构建文本集和标签"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read text data, cats.\n",
    "# the file path consists of tab separated sentences and cats.\n",
    "texts = []\n",
    "cats = []\n",
    "fh = open(training_data_path)\n",
    "for line in fh:\n",
    "    text, sentiment = line.split(\"\\t\")\n",
    "    texts.append(text)\n",
    "    cats.append(sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m-WjFyC6b_IE",
    "outputId": "5df9e11b-6f8e-42b8-e198-6fe343293cc3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000000\n"
     ]
    }
   ],
   "source": [
    "# Inspect the model\n",
    "word2vec_vocab = w2v_model.vocab.keys()\n",
    "word2vec_vocab_lower = [item.lower() for item in word2vec_vocab]\n",
    "print(len(word2vec_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XEz30Jztb_IP",
    "outputId": "2169b2c9-e89f-439a-a23f-d322fb856841"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000 3000\n",
      "Good case, Excellent value.\n",
      "1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Inspect the dataset\n",
    "print(len(cats), len(texts))\n",
    "print(texts[1])\n",
    "print(cats[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. 预处理：移除stop words + digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MFOGaDTwb_Ig",
    "outputId": "7603e297-9167-43ec-c7da-46d82dc850ad"
   },
   "outputs": [],
   "source": [
    "# preprocess the text.\n",
    "# remove stop words and digits\n",
    "\n",
    "def preprocess_corpus(texts):\n",
    "    mystopwords = set(stopwords.words(\"english\"))\n",
    "    def remove_stops_digits(tokens):\n",
    "        # Nested function that lowercases, removes stopwords and digits from a list of tokens\n",
    "        return [token.lower() for token in tokens if token not in mystopwords and not token.isdigit()\n",
    "               and token not in punctuation]\n",
    "    #This return statement below uses the above function to process twitter tokenizer output further. \n",
    "    return [remove_stops_digits(word_tokenize(text)) for text in texts]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000 3000\n",
      "['good', 'case', 'excellent', 'value']\n",
      "1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "texts_processed = preprocess_corpus(texts)\n",
    "print(len(cats), len(texts_processed))\n",
    "print(texts_processed[1])\n",
    "print(cats[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. 计算每个document 的embedding -- document 中所有word embedding 的平均值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fXRiGtY1b_Iq",
    "outputId": "2d57a96f-8da8-4285-ca1e-2c617578b9e1"
   },
   "outputs": [],
   "source": [
    "# Creating a feature vector by averaging all embeddings for all sentences\n",
    "\n",
    "def embedding_feats(list_of_lists):\n",
    "    DIMENSION = 300\n",
    "    zero_vector = np.zeros(DIMENSION)\n",
    "    feats = []\n",
    "    for tokens in list_of_lists:\n",
    "        feat_for_this =  np.zeros(DIMENSION)\n",
    "        count_for_this = 0\n",
    "        for token in tokens:\n",
    "            if token in w2v_model:\n",
    "                feat_for_this += w2v_model[token]\n",
    "                count_for_this +=1\n",
    "        if count_for_this == 0:  # 如果没有valid token，放一个全零的向量\n",
    "            print('Error: no valid token in the document: ', tokens)\n",
    "            feats.append(feat_for_this) \n",
    "        else:\n",
    "            feats.append(feat_for_this/count_for_this)  # 一个document 的vector 是每个单词vector 相加的平均值        \n",
    "    return feats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: no valid token in the document:  ['10/10']\n",
      "Error: no valid token in the document:  ['10/10']\n",
      "3000\n"
     ]
    }
   ],
   "source": [
    "train_vectors = embedding_feats(texts_processed)\n",
    "print(len(train_vectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mr9IaQppb_Ix",
    "outputId": "13a84b5c-fde3-49f4-b156-5c2f36592b19"
   },
   "outputs": [],
   "source": [
    "# Take any classifier (LogisticRegression here, and train/test it like before.\n",
    "classifier = LogisticRegression(random_state=1234)\n",
    "\n",
    "# Training and testing data set split\n",
    "train_data, test_data, train_cats, test_cats = train_test_split(train_vectors, cats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chenwang/anaconda3/envs/tf/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.848\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          0\n",
      "       0.86      0.84      0.85       386\n",
      "          1\n",
      "       0.83      0.86      0.85       364\n",
      "\n",
      "   micro avg       0.85      0.85      0.85       750\n",
      "   macro avg       0.85      0.85      0.85       750\n",
      "weighted avg       0.85      0.85      0.85       750\n",
      "\n"
     ]
    }
   ],
   "source": [
    "classifier.fit(train_data, train_cats)\n",
    "print(\"Accuracy: \", classifier.score(test_data, test_cats))\n",
    "preds = classifier.predict(test_data)\n",
    "print(classification_report(test_cats, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k7wjLB8rb_JB"
   },
   "source": [
    "Not bad. With little efforts we got 84% accuracy. Thats a great starting model to have!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8z3tJJJkb_JB"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Word2Vec_Example.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
