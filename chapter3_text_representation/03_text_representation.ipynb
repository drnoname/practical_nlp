{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 3. Text Representation\n",
    "\n",
    "无论你使用多么好的模型，如果feature 很差，结果一定也不会好。\n",
    "\n",
    "> garbage in, garbage out.\n",
    "\n",
    "本章介绍，怎么构建好的feature，换句话说，如何把text 转化成numerical form (作为模型的输入). 这个过程称为feature engineering，在NLP 领域也称为Text Representation.  \n",
    "\n",
    "<img src=\"../figures/3-1.png\" alt=\"drawing\" width=\"600\"/>\n",
    "\n",
    "feature representation 对于任何数据(text, video, image, speech)都是必备的一个步骤。但是文本数据数字化是一个相对复杂的过程。\n",
    "\n",
    "例如图像，每张图片可以看作是一个或多个matrix, 每个像素点都是由n个数字来encode它对应channel 颜色的intensity 的（通常n=3，对于grayscale image，n=1）。如下图所示。video 也类似，可以看作是一个sequence collection of matrices. \n",
    "\n",
    "<img src=\"../figures/3-2.png\" alt=\"drawing\" width=\"600\"/>\n",
    "\n",
    "speech 本质上是一个波，所以通常是对wave 做sampling，然后存储sample 的amplitude.\n",
    "\n",
    "<img src=\"../figures/3-3.png\" alt=\"drawing\" width=\"600\"/>\n",
    "\n",
    "然后我们可以得到一下speech representation.\n",
    "\n",
    "<img src=\"../figures/3-4.png\" alt=\"drawing\" width=\"600\"/>\n",
    "\n",
    "从上面可以看出，mathematically representing images, video, and speech is straightforward. 但是text 就相对复杂一些。本章介绍4种text representation 的方法：\n",
    "\n",
    "- Basic vectorization approaches\n",
    "- Distributed representations\n",
    "- Universal language representation\n",
    "- Handcrafted features\n",
    "\n",
    "一般来说，如果要理解一句话的语义信息，包括以下几步：\n",
    "1. Break the sentence into lexical units such as lexemes, words, and phrases\n",
    "2. Derive the meaning for each of the lexical units\n",
    "3. Understand the syntactic (grammatical) structure of the sentence\n",
    "4. Understand the context in which the sentence appears\n",
    "\n",
    "因此，我们可以定义一个好的text representation 方法一定是能够在语义上区分text 的。\n",
    "\n",
    "在NLP 领域，好的text presentation 比好的算法更有效。\n",
    "\n",
    "#### Vector space models\n",
    "\n",
    "represent text unit as vectors, compare the text similarity using cosine similarity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "## 1 Basic Vectorization Approaches\n",
    "\n",
    "1. map each word in the vocabulary (V) of the text corpus to a unique ID (integer value)\n",
    "2. then represent each sentence or document in the corpus as a V-dimensional vector. \n",
    "\n",
    "假设我们的corpus 有下面4个documents:\n",
    "\n",
    "- D1\tDog bites man.\n",
    "- D2\tMan bites dog.\n",
    "- D3\tDog eats meat.\n",
    "- D4\tMan eats food.\n",
    "\n",
    "我们首先做一个case normalization，然后移除标点，做tokenization，得到6个distinct tokens，构成我们的vocabulary: dog, bites, man, eats, meat, food. 注意，vocabulary 中单词的顺序不重要。这里，我们按照单词出现的顺序。\n",
    "\n",
    "下面，每个document 可以使用一个长度为6的vector 来表示。下面我们介绍几种方法来生产这个长度为6的vector.\n",
    "\n",
    "### 1.1 One-hot encoding\n",
    "\n",
    "每个document 是一个二维矩阵:\n",
    "- 一共有n个向量，n是document 中token 的个数。\n",
    "- 每个向量长度为|V|，只有一个为1，表示这个token 在字典中的位置，其余为0\n",
    "\n",
    "例如，上面D1 可以表示为下面一个3*6的矩阵。\n",
    "\n",
    "| D1    | dog | bites | man | eats | meat | food |\n",
    "|-------|-----|-------|-----|------|------|------|\n",
    "| dog   | 1   | 0     | 0   | 0    | 0    | 0    |\n",
    "| bites | 0   | 1     | 0   | 0    | 0    | 0    |\n",
    "| man   | 0   | 0     | 1   | 0    | 0    | 0    |\n",
    "\n",
    "#### pros\n",
    "- 简单直观，容易理解\n",
    "\n",
    "#### cons\n",
    "- sparse representation: 每一行只有一个1，其余全是0\n",
    "    - store，compute 成本高\n",
    "    - sparse 容易引起overfitting\n",
    "- 没有一个fix-length representation: \n",
    "    - 每个document 的representation 长度取决于token 的个数\n",
    "    - 许多算法要求输入的格式相同\n",
    "- 任意两个token 都是正交的，无法表示语义的距离\n",
    "    - poor at capturing the meaning of the word\n",
    "- out-of-vocabulary (OOV problem): 遇到了字典中没有的词，处理很麻烦，更新词典成本大\n",
    "    \n",
    "基于上述问题，如今one-hot encoding 很少使用了。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Bag of words (BoW)\n",
    "\n",
    "上述有些问题，可以通过bag of words 解决。bag of word 通常被用于**classification** problems.\n",
    "\n",
    "idea: \n",
    "- 一个document 看作一个bag (collection) of words.\n",
    "- ignore order and context.\n",
    "\n",
    "这个方法的假设是: 使用相似单词的documents 通常表示相同的意思。by analyzing the words present in a piece of text, one can identify the class (bag) it belongs to.\n",
    "\n",
    "每个document 是一个长度为|V| 的向量，第i个element 表示vocabulary 中第i个token 在这个document 中出现的次数，可以大于1.\n",
    "\n",
    "|tokens | dog | bites | man | eats | meat | food |\n",
    "|-------|-----|-------|-----|------|------|------|\n",
    "| dog   | 1   | 0     | 0   | 0    | 0    | 0    |\n",
    "| bites | 0   | 1     | 0   | 0    | 0    | 0    |\n",
    "| man   | 0   | 0     | 1   | 0    | 0    | 0    |\n",
    "| D1    | 1   | 1     | 1   | 0    | 0    | 0    |\n",
    "\n",
    "#### Improvement: set of words \n",
    "\n",
    "和bag of words 的区别是，每个element 只能是1或0， which indicates whether a word exists in the text or not. Researchers have shown that such a representation without considering frequency is useful for **sentiment analysis**.\n",
    "\n",
    "#### pros\n",
    "\n",
    "- simple to understand and implement\n",
    "- 可以在欧式空间捕获部分语义信息\n",
    "- fixed length encoding\n",
    "\n",
    "#### cons\n",
    "\n",
    "- sparsity. 解决方法：最常用的n 个词来减少vocabulary 的大小\n",
    "- 单词层面没有捕获任何语义信息，因为每个单词没有一个vector 来表示。\n",
    "- out-of-vocabulary\n",
    "- word order information is lost: 例如下面两句话的representation 完全一样，单其实是表示不同的意思。\n",
    "    - I love you\n",
    "    - you love me\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Bag of N-Grams (BoN)\n",
    "\n",
    "breaking text into chunks of n contiguous words (or tokens). 好处是可以capture 一些context 信息(单词顺序)。\n",
    "\n",
    "区别在于，字典由distinct tokens 变成distinct n-grams. 然后计算的方法和bag of words 完全相同。 \n",
    "\n",
    "#### 2-gram (bigram)\n",
    "\n",
    "还用上面的例子，我们的vocabulary 有8个distinct bigrams，|V|=8. 则D1和D2 如下表所示。\n",
    "\n",
    "| Document | dog bites | bites man | man bites | bites dog | dog eats | eats meat | man eats | eats food |\n",
    "|----------|-----------|-----------|-----------|-----------|----------|-----------|----------|-----------|\n",
    "| D1       | 1         | 1         | 0         | 0         | 0        | 0         | 0        | 0         |\n",
    "| D2       | 0         | 0         | 1         | 1         | 0        | 0         | 0        | 0         |\n",
    "\n",
    "可以看出，Bag of words 是bag of ngrams 的一种特殊情况，where n=1. 在NLP 领域，n-gram 也被称作\"n-gram feature selection\"。\n",
    "\n",
    "#### pros\n",
    "\n",
    "- capture context information\n",
    "    - 更好地捕捉语义信息\n",
    "\n",
    "#### cons\n",
    "- 随着n的增大，sparsity 的问题更加严重\n",
    "- out-of-vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 TF-IDF\n",
    "\n",
    "之前的三种方法，每一个token 都同等重要，然而，有时在表达语义方面，有些token 会扮演更重要的角色。\n",
    "\n",
    "TF-IDF (term frequency–inverse document frequency) aims to **quantify the importance of a given word relative to other words in the document and in the corpus**. 常用语information retrieval systems, 从一个corpus 中查找相关documents.\n",
    "\n",
    "- 如果一个单词在一个document 中出现了很多次，但是在其他documents 中出现的次数很少，可能说明这个单词对这个document 很重要。\n",
    "- 如果一个单词在其他documents 中出现的频率很高，那么说明这个单词可能是一个通用词，所以它对于当前document 的权重可能不高。\n",
    "\n",
    "#### Term Frequency (TF)\n",
    "评估一个单词对于一个document 的重要性。计算公式：频次除以总个数。\n",
    "\n",
    "$$TF ( t , d ) = \\frac{\\text{Number of occurrences of term  t  in document  d} } { \\text{Total number of terms in the document  d}}$$\n",
    "\n",
    "#### Inverse Document Frequency (IDF)\n",
    "\n",
    "the importance of the term across a corpus. 如果只看TF，stop words 肯定会有很高的权重，然而它们在很多时候并不是特别有用。所以IDF 是用来区分不同词的稀有性（重要性）的。\n",
    "\n",
    "$$IDF ( t ) = log \\frac{ \\text{Total number of documents in the corpus} } { \\text{Number of documents with term  t  in them}  }$$\n",
    "\n",
    "#### TF-IDF\n",
    "\n",
    "$$TF-IDF = TF * IDF $$\n",
    "\n",
    "用我们之前的例子：\n",
    "\n",
    "- D1: Dog bites man.\n",
    "- D2: Man bites dog.\n",
    "- D3: Dog eats meat.\n",
    "- D4: Man eats food.\n",
    "\n",
    "下面，我们计算D1 的TF-IDF:\n",
    "\n",
    "| Word  | TF score     | IDF score          | TF-IDF score          |\n",
    "|-------|--------------|--------------------|-----------------------|\n",
    "| dog   | $\\frac{1}{3}$ = 0.33 | $log_2\\frac{4}{3}$ = 0.415 | 0.415 * 0.33 = 0.137 |\n",
    "| bites | $\\frac{1}{3}$ = 0.33 | $log_2\\frac{4}{2}$ = 1     | 1* 0.33 = 0.33       |\n",
    "| man   | $\\frac{1}{3}$ = 0.33 | $log_2\\frac{4}{3}$ =0.415  | 0.415 * 0.33 = 0.137 |\n",
    "| eats  | $\\frac{0}{3}$ = 0.00 | $log_2\\frac{4}{2}$ =1      | 0 * 1 = 0            |\n",
    "| meat  | $\\frac{1}{3}$ = 0.00 | $log_2\\frac{4}{1}$ =2      | 0 * 2 = 0            |\n",
    "| food  | $\\frac{1}{3}$ = 0.00 | $log_2\\frac{4}{1}$ =2      | 0 * 2 = 0            |\n",
    "\n",
    "所以D1 的vector representation 是：0.137, 0.33, 0.137, 0, 0, 0\n",
    "\n",
    "注意，我们按照上述公式计算出的TF-IDF 结果，可能和`scikit-learn` 不同。因为scikit-learn 使用了一个modified version 来处理以下情况：\n",
    "1. 如果一个单词在所有document 中都出现，则IDF = log(1) = 0,则TF-IDF 为0. scikit-learn 不会ignore terms that appear in all documents.\n",
    "2. avoid possible zero division.\n",
    "\n",
    "#### Pros and Cons\n",
    "\n",
    "常用于: information retrieval and text classification. 总体来说，表现要比前面几种representation 要好。通常作为最初始的基线版本。但是，TF-IDF 还是有以下一些问题：\n",
    "\n",
    "1. discrete representation: 没有描述单词之间的关系。\n",
    "2. sparsity\n",
    "3. out-of-vocabulary\n",
    "\n",
    "下面我们介绍distributed representations 来解决上述问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Distributed Representations\n",
    "\n",
    "Aim to use neural network architectures to create dense, low-dimensional representations of words and texts.\n",
    "\n",
    "#### distributional similarity\n",
    "\n",
    "connotation: meaning is defined by context.\n",
    "\n",
    "#### distributional hypothesis\n",
    "\n",
    "hypothesis: words that occur in similar contexts have similar meanings. \n",
    "\n",
    "可以理解为可替换原则。\n",
    "\n",
    "- I want an orange juice.\n",
    "- I want an apple juice.\n",
    "\n",
    "由此可以看出，orange 和apple 的语义很接近（同为可以榨汁的水果）。\n",
    "\n",
    "#### distributional representation\n",
    "\n",
    "These vectors are obtained from a co-occurrence matrix that captures co-occurrence of word and context. \n",
    "\n",
    "我们之前介绍的4种方法：one-hot, bag of words, bag of n-grams, and TF-IDF 都属于这一类。\n",
    "\n",
    "#### Distributed representation\n",
    "\n",
    "降维: distributed representation schemes significantly compress the dimensionality. compact and dense vectors.\n",
    "\n",
    "#### Embedding\n",
    "\n",
    "embedding is a **mapping** between vector space coming from distributional representation to vector space coming from distributed representation.\n",
    "\n",
    "#### Vector semantics\n",
    "\n",
    "This refers to the set of NLP methods that aim to learn the word representations based on distributional properties of words in a large corpus.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Word embeddings\n",
    "\n",
    "analogies questions: king to man is equal to ___ to woman?\n",
    "\n",
    "> King - Man + Woman ≈ Queen\n",
    "\n",
    "distributed representation 有以下特点:\n",
    "- low dimensional: 50–500\n",
    "- dense: most values in these vectors are non-zero\n",
    "\n",
    "Word2vec: projecting the meaning of the words in a vector space where words with similar meanings will tend to cluster together, and words with very different meanings are far from one another."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Pre-trained word embeddings\n",
    "\n",
    "我们通常不需要自己去训练embedding，使用已经训练好的embedding 其实够用。如果我们要训练自己的embeddings，pre-trained embeddings 可以作为一个很好的baseline.\n",
    "\n",
    "pre-trained embeddings can be thought of as a large collection of key-value pairs, where keys are the words in the vocabulary and values are their corresponding word vectors.\n",
    "\n",
    "- word2vec / google\n",
    "- glove / stanford\n",
    "- fasttext / facebook\n",
    "\n",
    "dimension 一般是25, 50, 100, 200, 300, 600\n",
    "\n",
    "注意，如果一个token没有在vocabulary 中（例如practicalnlp），则会得到一个key not found exception, 所以好的实践是在获取之前先判断目标token 包含在vocabulary 中（类似于字典中获取key 对应的value）."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Training our own embeddings\n",
    "\n",
    "#### CBOW: Continous bag of words\n",
    "\n",
    "primary task is to build a **language model** that correctly predicts the center word given the context words in which the center word appears.\n",
    "\n",
    "language model is a (statistical) model that tries to give a probability distribution over sequences of words.\n",
    "The objective of a language model is to assign probabilities in such a way that it gives high probability to “good” sentences and low probabilities to “bad” sentences. \n",
    "By good, we mean sentences that are semantically and syntactically correct. By bad, we mean sentences that are incorrect—semantically or syntactically or both. \n",
    "- good sentence: The cat jumped over the dog -- probability close to 1.0\n",
    "- bad sentence: jumped over the the cat dog -- probability close to 0.0\n",
    "\n",
    "下图展示了一个context size = 2 的场景。\n",
    "\n",
    "<img src=\"../figures/3-7.png\" alt=\"drawing\" width=\"500\"/>\n",
    "\n",
    "CBOW 对与corpus 中的每一个词都做这样的预测。使用一个宽度为2k+1 的sliding window 构建训练集，如下图所示。\n",
    "\n",
    "<img src=\"../figures/3-8.png\" alt=\"drawing\" width=\"600\"/>\n",
    "\n",
    "之后，我们构建一个shallow network，如下图所示。\n",
    "\n",
    "<img src=\"../figures/3-9.png\" alt=\"drawing\" width=\"600\"/>\n",
    "\n",
    "The objective is to learn an embedding matrix $E_{|V| x d}$. \n",
    "\n",
    "注意：最后E 和E' 是两种embedding，可以使用任何一个，也可以使用两者的average.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Skip-Gram\n",
    "\n",
    "the task is to predict the context words from the center word，如下图所示。\n",
    "\n",
    "<img src=\"../figures/3-10.png\" alt=\"drawing\" width=\"500\"/>\n",
    "\n",
    "Skip-gram 构建training dataset 过程和CBOW 不同，每个sliding window 会生成2K 个数据点(pair), 如下图所示。\n",
    "\n",
    "<img src=\"../figures/3-11.png\" alt=\"drawing\" width=\"500\"/>\n",
    "\n",
    "Network 结构如下：\n",
    "\n",
    "<img src=\"../figures/3-12.png\" alt=\"drawing\" width=\"500\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### hyperparameters: \n",
    "- window size\n",
    "- dimensionality of the vectors to be learned\n",
    "- learning rate\n",
    "- number of epochs\n",
    "- etc. \n",
    "\n",
    "参数调教对性能影响很大。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Out-of-vocabulary 问题\n",
    "\n",
    "\n",
    "#### 方法1: 丢弃\n",
    "\n",
    "一个方法是，丢弃不在vocabulary 中的单词。\n",
    "- Assumption: embeddings 通常是基于大corpus 训练的，因此OOV 的概率应该很低。\n",
    "- 如果embedding vocabulary 和production vocabulary 的overlap 小于80%，performance 通常会不好\n",
    "- 此外，还要看不重合的词是什么词，如果我们应用到一个新的domain，例如：医学，日志分析，则很多关键的词在embedding vacabulary 中都没有，则会大大影响model 的性能。\n",
    "\n",
    "#### 方法2: 随机创建新的vector\n",
    "\n",
    "每个element在-0.25 ～ 0.25之间。经验上看，这个对performance 会提升1%-2%。\n",
    "\n",
    "#### 方法3: sub-word information\n",
    "\n",
    "FastText: learns embeddings for words and character n-grams together and views a word’s embedding vector as an aggregation of its constituent character n-grams.\n",
    "\n",
    "因此，对于没有见过的单词，可以拆成n-grams，然后找这些n-gram 的vector是，再聚合起来。\n",
    "例如，gregarious 可以拆成gre, reg, ega, ….ous，然后把这些trigrams 的vectors 聚合起来计算gregarious 这个词的embedding.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5. Going beyond words\n",
    "\n",
    "我们现在已经对每个word 有一个embedding，然而在实际应用场景中，我们很少去看每个单词，更多是对句子或者document 进行理解。下面我们介绍一下，如何present larger units of text.\n",
    "\n",
    "#### 方法1\n",
    "\n",
    "把text 分解成单词，然后把每个单词的word vector 聚合在一起，聚合的方法有：\n",
    "- sum\n",
    "- avg\n",
    "- etc.\n",
    "\n",
    "这种方法可能带来问题，因为忽略了词的顺序。例如：按照上面的方法，I love you 和you love me 两句话的vector 是相同的。\n",
    "\n",
    "然而，在实际应用中效果很好。CBOW 就是一个例子，把4个context word 的input vector 相加得到一个vector 来来表context.\n",
    "\n",
    "#### Doc2vec\n",
    "\n",
    "为了解决上面忽略context 的问题，Doc2vec directly learn the representations for texts of arbitrary lengths (phrases, sentences, paragraphs, and documents) by taking the context of words in the text into account.\n",
    "\n",
    "下图显示了两个network structures:\n",
    "- Distributed memory (DM) -- 左\n",
    "- Distributed Bag of Words DBOW -- 右\n",
    "\n",
    "<img src=\"../figures/3-13.png\" alt=\"drawing\" width=\"600\"/>\n",
    "\n",
    "Doc2vec 常用语text classification, document tagging, text recommendation systems, and simple chatbots for FAQs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Universal Text Representations\n",
    "\n",
    "到现在为止，我们看到的是一个word 对应一个vector representation. 这会产生一些问题，因为Words can mean different things in different contexts（多义词）.\n",
    "\n",
    "2018 年之后，研究领域提出了contextual word representations.\n",
    "- ELMo\n",
    "- BERT\n",
    "- GPT\n",
    "\n",
    "这些model leverage transfer learning:\n",
    "- learning embeddings on a generic task on massive corpus\n",
    "- fine-tune on task-specific data\n",
    "\n",
    "这些model 在许多NLP 的task 上都表现的非常好，例如question answering, semantic role labeling, named entity recognition, coreference resolution, etc.\n",
    "\n",
    "### 使用word vector 的一些经验：\n",
    "- 所有的representation 都是biased. 例如如果我们训练的是一个Tech news， Apple 可能和Microsoft， Google 比较接近，和Orange 比较远。biases stemming from training data can have serious implications on the performance of NLP models and systems.\n",
    "    - 理解和修正bias\n",
    "    - ref: Bolukbasi, Tolga, Kai-Wei Chang, James Y. Zou, Venkatesh Saligrama, and Adam T. Kalai. “Man Is to Computer Programmer as Woman Is to Homemaker? Debiasing Word Embeddings.” Advances in Neural Information Processing Systems 29 (NIPS 2016): 4349–4357.\n",
    "    \n",
    "- pre-trained model 一般比较大，～4.5GB。\n",
    "    - engineering 通常使用redis with a cache on top of them to address scaling and latency issues.\n",
    "    - load embeddings into database, and use them as if they are available in RAM.\n",
    "    \n",
    "- embedding 不能解决所有问题，例如sarcasm（讽刺），还有一些说反话等。\n",
    "\n",
    "- 在产品中使用，除了要使用最新的model 和技术之外，通常还要考虑一些其他的因素（practical issues），例如：\n",
    "    - return on investment from the effort\n",
    "    - business needs\n",
    "    - infrastructural constraints \n",
    "    - etc.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualizing embeddings\n",
    "\n",
    "通常对于一个问题的理解，来源于对数据的分析。visual exploration 对于理解NLP 问题很有帮助。如果想要visually inspect word vectors，我们首先需要将它们投影到低维空间。\n",
    "\n",
    "t-SNE (t-distributed Stochastic Neighboring Embedding) 是word embedding 常用的降维方法。\n",
    "\n",
    "下图显示了一些词的embedding. 可以看出左半部分的词多为计数的，右半部分多为职业。\n",
    "\n",
    "<img src=\"../figures/3-15.png\" alt=\"drawing\" width=\"600\"/>\n",
    "\n",
    "下图显示了analogy 的问题。箭头捕获了relationship between words (例如，性别)。\n",
    "\n",
    "<img src=\"../figures/3-16.png\" alt=\"drawing\" width=\"600\"/>\n",
    "\n",
    "t-SNE 也同样可以用于document embedding，如下图所示。我们把Wikipedia 的documents 按topic 分类，计算每个document 的vector，然后打印出来，我们可以看出颜色相同的基本都离的很近。\n",
    "\n",
    "<img src=\"../figures/3-17.png\" alt=\"drawing\" width=\"600\"/>\n",
    "\n",
    "#### tensorbord\n",
    "\n",
    "也可以使用tensorboard，具体使用参见../10_Visualizing_Embeddings_using_Tensorboard.ipynb\n",
    "\n",
    "<img src=\"../figures/3-18.png\" alt=\"drawing\" width=\"600\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Handcrafted Feature Representations\n",
    "\n",
    "一些特殊的应用场景，例如automated essay scorer (used in GRE and TOEFL).\n",
    "Evaluating an essay for various aspects of writing requires specialized features that address those needs.\n",
    "\n",
    "这些应用场景都十分复杂。vectorization approaches are more accessible to get started with, especially when we don’t have enough understanding of the domain. \n",
    "\n",
    "heuristics, handcrafted features, and ML are all used together in real-world industrial systems dealing with NLP problems. 后面我们也会介绍，如何在ML 中加入一些handcrafted features.\n",
    "\n",
    "- Chiticariu, L., Yunyao Li, and Frederick R. Reiss. “Rule-Based Information Extraction is Dead! Long Live Rule-Based Information Extraction Systems!” Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing (2013): 827–832.\n",
    "\n",
    "- Suganthan G.C., Paul, Chong Sun, Haojun Zhang, Frank Yang, Narasimhan Rampalli, Shishir Prasad, Esteban Arcaute, et al. “Why Big Data Industrial Systems Need Rules and What We Can Do About It.” Proceedings of the 2015 ACM SIGMOD International Conference on Management of Data (2015): 265–276.\n",
    "\n",
    "#### 什么时候使用vectors，什么时候使用hand-crafted features？\n",
    "depends on the task.\n",
    "\n",
    "- text classification:\n",
    "    - word embedings\n",
    "- information retrieval:\n",
    "    - crafted features\n",
    "    \n",
    "很多场景下，需要使用hybrid 方法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
